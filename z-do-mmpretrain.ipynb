{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首先，当然是“喜闻乐见”的环境配置环节\n",
    "我的环境 win11 \\\n",
    "建议命令行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n mmpretrain python=3.9\n",
    "# python版本和视频演示一致\n",
    "\n",
    "# 安装最好带上-c nvidia 不然非常容易产生冲突，不知道咋形成的。\n",
    "!conda activate mmpretrain\n",
    "!conda install pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# 不知道torchvision和torchaudia有不有用，先装了再说\n",
    "\n",
    "# 克隆仓库，网页被重定向了就记得挂梯子\n",
    "## 模块安装的好处：方便，简单，适合只是调用其中模块或者API\n",
    "## 克隆好处：方便使用其中脚本，方便查看和修改其中模块，模型结构\n",
    "!git clone https://github.com/open-mmlab/mmpretrain\n",
    "\n",
    "# 安装MIM，这是OpenMMLab的包管理工具，用这个就犯不着冲突的问题。\n",
    "!pip install openmim\n",
    "\n",
    "# 安装mmpretrain算法库\n",
    "!cd mmpretrain\n",
    "!mim install -e \".[multimodal]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看安装是否成功\n",
    "记一些报错：\n",
    "1、显示没有安装ipykernel，因为我是用vscode，然后jupyter方式运行。 \\\n",
    "但是在弹出框点击“安装”和按照报错提示使用命令`conda install -n mmpretrain ipykernel --update-deps --force-reinstall`都没有效果，任然提示要求安装ipykernel。 \\\n",
    "解决方法：直接命令行输入`conda install ipykernel`就解决。\\\n",
    "\n",
    "2、报错“No module named 'importlib_metadata'”\n",
    "直接`pip install importlib-metadata`即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0rc8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看安装的版本\n",
    "import mmpretrain\n",
    "\n",
    "mmpretrain.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_model —— 模型的获取 \\\n",
    "list_models —— 模型的列举 \\\n",
    "inference_model —— 模型的推理 \\\n",
    "\n",
    "注意：get_model拿到的模型未加载预训练权重，不能直接用于模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet18_8xb16_cifar10', 'resnet18_8xb32_in1k']\n",
      "-----------\n",
      "<class 'mmpretrain.models.classifiers.image.ImageClassifier'>\n",
      "<class 'mmpretrain.models.backbones.resnet_cifar.ResNet_CIFAR'>\n"
     ]
    }
   ],
   "source": [
    "from mmpretrain import get_model, list_models, inference_model\n",
    "\n",
    "# 列车图片分类任务中，resnet18模型有哪些\n",
    "print(list_models(task='Image Classification', pattern='resnet18'))\n",
    "\n",
    "print('-----------')\n",
    "\n",
    "# 模型实例化\n",
    "model = get_model('resnet18_8xb16_cifar10')\n",
    "print(type(model))\n",
    "print(type(model.backbone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blip-base_3rdparty_caption', 'blip2-opt2.7b_3rdparty-zeroshot_caption']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'BlipTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v1/blip/blip-base_3rdparty_coco-caption_20230419-a5b71af3.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'pred_caption': 'a close up of a lizard on the ground'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型需要传入预训练参数，以图片描述任务为例\n",
    "print(list_models(task='Image Caption', pattern='blip'))\n",
    "# 第二个参数是图片路径，我找了一个蜥蜴图片\n",
    "inference_model('blip-base_3rdparty_caption', 'xiyi.jpg', show=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个实例，猫和狗的分类任务\n",
    "## 一、下载数据集\n",
    "下载链接：（未给出）就不尝试了。\n",
    "\n",
    "目录：\n",
    "* training_set\n",
    "* val_set\n",
    "* test_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据自己下载，然后解压到data文件夹下，data文件夹自己创建\n",
    "# 查看一下目录结构 linux下\n",
    "\n",
    "cd ./data/cats_dogs_dataset\n",
    "tree ./ --filelimit=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置文件\n",
    "configs目录下面是各种模型配置文件示例。\\\n",
    "配置文件的核心——用字典的形式进行所有配置的设置。 \\\n",
    "进入配置文件，发现关联了四个文件:\n",
    "```\n",
    "_base_ = [\n",
    "    '../_base_/models/resnet18_cifar.py', '../_base_/datasets/cifar10_bs16.py',\n",
    "    '../_base_/schedules/cifar10_bs128.py', '../_base_/default_runtime.py'\n",
    "]\n",
    "```\n",
    "这里使用了OpenMMLab里面的继承机制，基层机制不详细讲解。\\\n",
    "可以了解的是，这里相当于**把四个配置未见里面的所有配置进行了综合**。\n",
    "\n",
    "## model配置文件\n",
    "可以查看一下`../_base_/models/resnet18_cifar.py`中的内容：\n",
    "\n",
    "```python\n",
    "# model settings\n",
    "model = dict(\n",
    "\n",
    "    # 用于定义model的类型\n",
    "    type='ImageClassifier',\n",
    "\n",
    "    # 主干网络，模型的核心\n",
    "    backbone=dict(\n",
    "        type='ResNet_CIFAR',\n",
    "        depth=18,\n",
    "        num_stages=4,\n",
    "        out_indices=(3, ),\n",
    "        style='pytorch'),\n",
    "    \n",
    "    # 颈，连接主干网络和头\n",
    "    neck=dict(type='GlobalAveragePooling'),\n",
    "\n",
    "    # 头，一般是模型最后的部分，例如有预分类头\n",
    "    head=dict(\n",
    "        type='LinearClsHead',\n",
    "\n",
    "        # num_classes表示类别，如果二分类，就为2\n",
    "        num_classes=10,\n",
    "        in_channels=512,\n",
    "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
    "    ))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理部分配置文件\n",
    "\n",
    "# dataset settings\n",
    "```python\n",
    "dataset_type = 'CIFAR10'\n",
    "data_preprocessor = dict(\n",
    "    num_classes=10,\n",
    "    # RGB format normalization parameters\n",
    "    mean=[125.307, 122.961, 113.8575],\n",
    "    std=[51.5865, 50.847, 51.255],\n",
    "    # loaded images are already RGB format\n",
    "    # 因为OpenCV得到的是BGR格式\n",
    "    to_rgb=False)\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='RandomCrop', crop_size=32, padding=4),\n",
    "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
    "    dict(type='PackInputs'),\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='PackInputs'),\n",
    "]\n",
    "\n",
    "# 数据集加载部分\n",
    "train_dataloader = dict(\n",
    "    batch_size=16,\n",
    "    # 加载样本的进程数\n",
    "    num_workers=2,\n",
    "    # 训练数据集的配置\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "\n",
    "        # 路径，注意一般训练集、测试集这些划分和数据集类型有关，也就是上面dataset_type = 'CIFAR10'字段\n",
    "        data_root='data/cifar10',\n",
    "        split='train',\n",
    "        pipeline=train_pipeline),\n",
    "    # 采样器的配置\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    ")\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root='data/cifar10/',\n",
    "        split='test',\n",
    "        pipeline=test_pipeline),\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    ")\n",
    "\n",
    "# topk参数表示计算top-n的准确率，如果是二分类，就只能top-1准确率\n",
    "val_evaluator = dict(type='Accuracy', topk=(1, ))\n",
    "\n",
    "test_dataloader = val_dataloader\n",
    "test_evaluator = val_evaluator\n",
    "```\n",
    "\n",
    ">>tips:\n",
    "1. 如果已经加载过配置文件了，再修改`dataset_type`字段，那么下面`train_dataloader`里面的`dataset_type`并不会髓质改变，因为配置文件加载一次之后，字段之间的连接关系就没有了。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规划配置\n",
    "\n",
    "schedules —— 训练、验证、测试的流程是怎么样的\n",
    "```python\n",
    "\n",
    "# 配置优化器，还可以在这里配置其他优化器参数\n",
    "# optimizer\n",
    "# 微调的话，可以把lr再拉小一点，比如0.01\n",
    "optim_wrapper = dict(\n",
    "    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001))\n",
    "\n",
    "# 指定在100，150个epoch处降低学习率\n",
    "# learning policy\n",
    "param_scheduler = dict(\n",
    "    type='MultiStepLR', by_epoch=True, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# train, val, test setting\n",
    "# 训练、验证、测试的流程设置\n",
    "# 空表示按默认配置进行\n",
    "# train_cfg里面的设置表示：整个训练流程总共进行200个epoch，每个epoch完成之后就进行一次验证\n",
    "# 如果加载了预训练权重，那么训练几轮就好了，应为会收敛很快。\n",
    "train_cfg = dict(by_epoch=True, max_epochs=200, val_interval=1)\n",
    "val_cfg = dict()\n",
    "test_cfg = dict()\n",
    "\n",
    "# NOTE: `auto_scale_lr` is for automatically scaling LR\n",
    "# based on the actual training batch size.\n",
    "# batch_size越小，就需要越小的学习率。\n",
    "# 注意如果是多卡，这里是所有PGU上所有batch_size加和\n",
    "auto_scale_lr = dict(base_batch_size=128)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行时间文件 \n",
    "\n",
    "default_runtime.py\n",
    "```python\n",
    "# defaults to use registries in mmpretrain\n",
    "default_scope = 'mmpretrain'\n",
    "\n",
    "# configure default hooks\n",
    "default_hooks = dict(\n",
    "    # record the time of every iteration.\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "\n",
    "    # print log every 100 iterations. interval代表间隔多少次迭代，打印一次日志。\n",
    "    logger=dict(type='LoggerHook', interval=100),\n",
    "\n",
    "    # enable the parameter scheduler.\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "\n",
    "    # save checkpoint per epoch. 没interval次，保留一次权重。\n",
    "    # 还可以添加： max_kkep_ckpts=5, save_best='auto'  分别表示：保留最后5个checkpoints， 自动根据验证集上结果，保存到目前为止验证集上效果最好的模型\n",
    "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
    "\n",
    "    # set sampler seed in distributed evrionment.\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "\n",
    "    # validation results visualization, set True to enable it.\n",
    "    visualization=dict(type='VisualizationHook', enable=False),\n",
    ")\n",
    "\n",
    "# configure environment\n",
    "env_cfg = dict(\n",
    "    # whether to enable cudnn benchmark\n",
    "    cudnn_benchmark=False,\n",
    "\n",
    "    # set multi process parameters\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "\n",
    "    # set distributed parameters\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "# set visualizer\n",
    "vis_backends = [dict(type='LocalVisBackend')]\n",
    "visualizer = dict(type='UniversalVisualizer', vis_backends=vis_backends)\n",
    "\n",
    "# set log level\n",
    "log_level = 'INFO'\n",
    "\n",
    "# load from which checkpoint\n",
    "load_from = None\n",
    "\n",
    "# whether to resume training from the loaded checkpoint\n",
    "resume = False\n",
    "\n",
    "# Defaults to use random seed and disable `deterministic`\n",
    "# 一般在训练的时候就指定了随机种子， deterministic是确定性增强选项，\n",
    "randomness = dict(seed=None, deterministic=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战\n",
    "\n",
    "## 第一步，配置文件\n",
    "可以直接将上述配置文件中内容复制粘贴到我们的配置文件，这样所需要的配置参数就得到了。  \n",
    "然后，根据需求修改配置文件：\n",
    "1. 加载预训练权重。  —— 其实就是一种特殊的模型参数初始化的方式。  \n",
    ">> tips：预训练权重：模型在其他大型数据集上，进行有监督或者无监督的方式提前训练好的参数。虽然，预训练使用的数据集和我们目标数据集不一样，但是模型学习到的提取图像特征是共通的。这样可以大大加速模型在我们目标训练集上的收敛速度。  \n",
    "\n",
    "模型预训练权重哪里可以找到：在MMPretrain的[官网](https://mmpretrain.readthedocs.io/en/latest/)，“model zoo”一栏，按照需求右键复制所需模型的下载链接。\n",
    "\n",
    "在model字典里面修改添加字段：\n",
    "```python\n",
    "init_cfg=dict(type='Pretrained', checkpoint='https://download.openmmlab.com/mmclassification/v0/beit/beit-base_3rdparty_in1k_20221114-c0a4df23.pth')\n",
    "```\n",
    "\n",
    "2. 根据需求修改配置文件\n",
    "\n",
    "##  开启训练\n",
    "1. 方式一：使用tools/下面的train文件。\n",
    "2. 可以使用`mim`在目录中任意位置启动训练：\\\n",
    "`mim train mmpretrain 配置文件.py --work-dir=./exp` --work-dir参数指定保存路径。  \n",
    "\n",
    "## 开启测试\n",
    "`mim test mmpretrain 配置文件.py --checkpoint 模型路径`\n",
    "\n",
    "保留每个测试样本的结构到result.pkl文件里\\\n",
    "`mim test mmpretrain 配置文件.py --checkpoint 模型路径 --out result.pkl`\n",
    "\n",
    "## 自带工具分析结果\n",
    "分析哪些是得分很高而且是正确的，哪些得分很高但是分类错误\\\n",
    "`mim run mmpretrain analyze_results 配置文件.py result.pkl --out-dir analyze`\n",
    "\n",
    "得到混淆矩阵\\\n",
    "`mim run mmpretrain confusion_matrix 配置文件.py result.pkl --show --include-values`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后，在一个真实图片上进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmpretrain import ImageClassificationInferencer\n",
    "\n",
    "inferencer = ImageClassificationInferencer('配置文件.py', pretrained='模型路径')\n",
    "inferencer(\"图片路径\", show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
